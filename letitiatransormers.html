<!DOCTYPE html>
<html lang="en"><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Letitiacap</title>
    <style>
        body {
            font-family: sans-serif;
            font-size: 18px;
            color: #111;
            padding: 0 0 1em 0;
        }
        .l {
          color: #050;
        }
        .s {
            display: inline-block;
        }
        .e {
            display: inline-block;
        }
        .t {
            display: inline-block;
        }
    </style>
  </head>
  <body>
    <h1>Letitiacap: Letitia Whisper captions</h1>
<div>
These are transcripts for AI Coffee Break with Letitia: Vision Transformers explained playlist.
First we scrapped the playlist <a href="https://www.youtube.com/playlist?list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6">web page</a>  to download the audio files. <br>
Then we transcribe them using <a href="https://github.com/openai/whisper">OpenAI Whisper</a>. The generated captions are using the small model. <br>
Download the raw captions data as a zip file <a href="https://drive.google.com/drive/folders/1ESF-Ln1W-TFEv_oiPdxJAl6f5BJiLut5?usp=sharing">here</a>.<br>
Send comments to <a href="https://twitter.com/A2theZ3">@h2thez</a> on Twitter.
</div>
    <h2>Videos:</h2>
    <div><a href="https://www.youtube.com/watch?v=DVoHvmww2lQ&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=1">1</a>  An image is worth 16x16 words: ViT | Is this the extinction of CNNs? Long live the Transformer?  &nbsp; | <a href = "https://drive.google.com/file/d/1c-Pp4OSlpynt1q-r0ENWW_5UKEYRuyO0/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=-FbV2KgRM8A&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=2">2</a>  Data-efficient Image Transformers EXPLAINED! Facebook AI's DeiT paper  &nbsp; | <a href = "https://drive.google.com/file/d/1VruVHW4vlBA7-9s5qytFDfH6WJ3_ON6B/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=HWna2c5VXDg&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=3">3</a>  Transformer in Transformer: Paper explained and visualized | TNT  &nbsp; | <a href = "https://drive.google.com/file/d/1hdL55aJiMx1cDI-e9ZUvfEbB5kf-RRIX/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=aH7s6qXEUcc&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=4">4</a>  Transformers can do both images and text. Here is why.  &nbsp; | <a href = "https://drive.google.com/file/d/1n2M3mwS06eeVGeJI-cKnmZLb-3tncl3i/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=SndHALawoag&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=5">5</a>  Swin Transformer paper animated and explained  &nbsp; | <a href = "https://drive.google.com/file/d/1ooYFOZ7IZ0F8c0X1vy69oUk7VzDt5VxH/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=XU4ZHUnnbpg&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=6">6</a>  SimVLM explained | What the paper doesn’t tell you  &nbsp; | <a href = "https://drive.google.com/file/d/11UwBBfRvYwtogdWfNpnWRIYk7rQq4deG/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=Dp6iICL2dVI&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=7">7</a>  Masked Autoencoders Are Scalable Vision Learners – Paper explained and animated!  &nbsp; | <a href = "https://drive.google.com/file/d/1Bn3ru0zW3AZC4pMmNncs_qzOmVMIyhIT/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=QqejV0LNDHA&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=8">8</a>  ConvNeXt: A ConvNet for the 2020s – Paper Explained (with animations)  &nbsp; | <a href = "https://drive.google.com/file/d/1JfQm0R79pOIRK99DMgkckfuV_digqoVz/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=dOwRXpSSc8E&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=9">9</a>  How do Vision Transformers work? – Paper explained | multi-head self-attention & convolutions  &nbsp; | <a href = "https://drive.google.com/file/d/1DQJoLwhE34nsadyx0OPWIawNUWeP_goj/view?usp=sharing"> caption </a> </div>
    <div><a href="https://www.youtube.com/watch?v=XHAoV_nKr1o&list=PLpZBeKTZRGPMddKHcsJAOIghV8MwzwQV6&index=10">10</a>  SEER explained: Vision Models more Robust & Fair when pretrained on UNCURATED images!?  &nbsp; | <a href = "https://drive.google.com/file/d/1GeQS9-Pta-2y178AWT8gM0IXpEf5IE93/view?usp=sharing"> caption </a> </div>

</body></html>